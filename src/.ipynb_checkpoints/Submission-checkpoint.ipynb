{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_submission import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, LSTM, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkWVxxUzq6FE"
   },
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 198492,
     "status": "ok",
     "timestamp": 1576688681781,
     "user": {
      "displayName": "Juliane Dervaux",
      "photoUrl": "",
      "userId": "05317504505871266746"
     },
     "user_tz": -60
    },
    "id": "fWx5LgJdq-HT",
    "outputId": "afa3b852-6520-4031-9230-7ef0b7b22608"
   },
   "outputs": [],
   "source": [
    "#NB : test and train dataset must be pre-processed before runing this file\n",
    "\n",
    "feat_vectors, labels, _, pca_model, _ = vectorization_train_submission(embedding_dim=350)\n",
    "test_vectors, _ = vectorization_test_submission(pca_model, 0, embedding_dim=350)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_vectors = scaler.fit_transform(feat_vectors)\n",
    "test_vectors = scaler.transform(test_vectors)\n",
    "\n",
    "loss_ = 'hinge'\n",
    "intercept = False\n",
    "regulariser = 0.5\n",
    "nb_features = feat_vectors.shape[1]\n",
    "\n",
    "classifier = svm.LinearSVC(fit_intercept= intercept ,loss=loss_,C=regulariser).fit(feat_vectors,labels)\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "print(predictions.shape)\n",
    "predictions = np.where(predictions==0, -1, predictions)\n",
    "\n",
    "ids_test = range(1, len(predictions)+1)\n",
    "\n",
    "\n",
    "# OUTPUT SUBMISSION FILE\n",
    "OUTPUT_PATH = '../results/BuzzLastyear_TweetPredictions_SVM.csv'\n",
    "create_csv_submission(ids_test, predictions, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkUyQzVXrBJb"
   },
   "source": [
    "## **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 288200,
     "status": "error",
     "timestamp": 1576719401250,
     "user": {
      "displayName": "Alice Bizeul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC-deTn6PKFL86g6O4nKPb1KRRcAZUWZn-uVCMMYQ=s64",
      "userId": "18284014108449347405"
     },
     "user_tz": -60
    },
    "id": "vwYDzJ4SqxT8",
    "outputId": "55a798bb-25ee-426c-dd25-8c80bda46294"
   },
   "outputs": [],
   "source": [
    "#NB : test and train dataset must be pre-processed before runing this file\n",
    "\n",
    "_, labels, feat_matrices, pca_model, maximal_length = vectorization_train_submission(embedding_dim=350)\n",
    "_, test_matrices = vectorization_test_submission(pca_model, maximal_length, embedding_dim=350)\n",
    "\n",
    "\n",
    "# Convert labels to 2 categorical variables, to be able to use categorical_crossentropy loss function\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_matrices, labels, test_size=0.1, random_state=1)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(420, dropout = 0.2, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(2, activation = 'softmax'))\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model_lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs = 260,\n",
    "    batch_size = 512,\n",
    "    validation_data = (X_test, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "predictions = model_lstm.predict(test_matrices)\n",
    "predictions=np.argmax(predictions, axis=1)\n",
    "predictions = np.where(predictions==0, -1, predictions)\n",
    "ids_test = range(1, len(predictions)+1)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "# OUTPUT SUBMISSION FILE\n",
    "OUTPUT_PATH = '../results/BuzzLastyear_TweetPredictions_LSTM.csv'\n",
    "create_csv_submission(ids_test, predictions, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftdjt9ZWrHGi"
   },
   "source": [
    "## **Classical-CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19133,
     "status": "ok",
     "timestamp": 1576359353326,
     "user": {
      "displayName": "Juliane Dervaux",
      "photoUrl": "",
      "userId": "05317504505871266746"
     },
     "user_tz": -60
    },
    "id": "oHWOCbF-tira",
    "outputId": "ab59cafb-0f78-43e5-fc96-2fd9a8d7b5a7"
   },
   "outputs": [],
   "source": [
    "# Load Test dataset\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, LSTM, SpatialDropout1D, GlobalMaxPooling1D, Input, concatenate, Activation, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#NB : test and train dataset must be pre-processed before runing this file\n",
    "\n",
    "_, labels, feat_matrices, pca_model, maximal_length = vectorization_train_submission(embedding_dim=30)\n",
    "_, test_matrices = vectorization_test_submission(pca_model, maximal_length, embedding_dim=30)\n",
    "\n",
    "# Convert labels to 2 categorical variables, to be able to use categorical_crossentropy loss function\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 260\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu')) #2-grams\n",
    "model.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu')) #3-grams\n",
    "#model.add(Conv1D(filters=size, kernel_size=4, padding='valid', activation='relu')) #4-grams\n",
    "#model.add(Conv1D(filters=size, kernel_size=5, padding='valid', activation='relu')) #5-grams\n",
    "#model.add(Conv1D(filters=size, kernel_size=6, padding='valid', activation='relu')) #6-grams\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(50, 10, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(50, 10, activation='relu', padding='same'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,\n",
    "                y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(X_test,y_test),\n",
    "                verbose=1)\n",
    "\n",
    "predictions = model_m.predict(test_matrices)\n",
    "predictions = np.where(predictions==0, -1, predictions)\n",
    "ids_test = range(1, len(predictions)+1)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "# OUTPUT SUBMISSION FILE\n",
    "OUTPUT_PATH = '../results/BuzzLastyear_TweetPredictions_ClassicalCNN.csv'\n",
    "create_csv_submission(ids_test, predictions, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-channel CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHwxrZkH_XLy"
   },
   "outputs": [],
   "source": [
    "# Load Test dataset\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape, Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, LSTM, SpatialDropout1D, GlobalMaxPooling1D, Input, concatenate, Activation, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#NB : test and train dataset must be pre-processed before runing this file\n",
    "\n",
    "_, labels, feat_matrices, pca_model, maximal_length = vectorization_train_submission(embedding_dim=30)\n",
    "_, test_matrices = vectorization_test_submission(pca_model, maximal_length, embedding_dim=30)\n",
    "\n",
    "# Convert labels to 2 categorical variables, to be able to use categorical_crossentropy loss function\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 260\n",
    "\n",
    "\n",
    "tweet_input = Input(shape=(MAXIMAL_TWEET_LENGTH, EMBEDDING_SIZE))\n",
    "\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_input)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_input)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "#fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_input)\n",
    "#fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "#fivegram_branch = Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu', strides=1)(tweet_input)\n",
    "#fivegram_branch = GlobalMaxPooling1D()(fivegram_branch)\n",
    "#merged = concatenate([bigram_branch, trigram_branch, fourgram_branch, fivegram_branch], axis=1)\n",
    "merged = concatenate([bigram_branch, trigram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(2)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(X_test,y_test),\n",
    "                verbose=1)\n",
    "\n",
    "predictions = model_m.predict(test_matrices)\n",
    "predictions = np.where(predictions==0, -1, predictions)\n",
    "ids_test = range(1, len(predictions)+1)\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "# OUTPUT SUBMISSION FILE\n",
    "OUTPUT_PATH = '../results/BuzzLastyear_TweetPredictions_ClassicalCNN.csv'\n",
    "create_csv_submission(ids_test, predictions, OUTPUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZkWVxxUzq6FE"
   ],
   "machine_shape": "hm",
   "name": "Submission.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
